{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNG/jKKQxuJvSpdk5UG04/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronaksingh27/IR_assgn5_pb2/blob/master/IR_assignment5_pb1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7Ji2aSowzhw",
        "outputId": "329a31b5-c925-48e3-d522-910107f26cc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1f1hh2fiwxqw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkXe7pqxw7ak",
        "outputId": "9589c7d7-4cb8-4eb7-f0fe-672647afc8f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_document(document):\n",
        "    \"\"\"Preprocess the text: tokenization, stopword removal, stemming.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize the document\n",
        "    tokens = word_tokenize(document.lower())\n",
        "\n",
        "    # Remove stopwords and apply stemming\n",
        "    processed_tokens = [stemmer.stem(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    return ' '.join(processed_tokens)\n"
      ],
      "metadata": {
        "id": "QwjyzWmpxEMb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_index(corpus_dir):\n",
        "    \"\"\"Build a TF-IDF index from a corpus directory.\"\"\"\n",
        "    documents = []\n",
        "    doc_names = []\n",
        "\n",
        "    for filename in os.listdir(corpus_dir):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(corpus_dir, filename), 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                preprocessed_content = preprocess_document(content)\n",
        "                documents.append(preprocessed_content)\n",
        "                doc_names.append(filename)\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    return tfidf_matrix, vectorizer, doc_names\n"
      ],
      "metadata": {
        "id": "SJ_joW5OxGva"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plagiarism_check(test_document, tfidf_matrix, vectorizer, doc_names):\n",
        "    \"\"\"Check plagiarism by comparing the test document to the corpus.\"\"\"\n",
        "    preprocessed_test_doc = preprocess_document(test_document)\n",
        "    test_vector = vectorizer.transform([preprocessed_test_doc])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_scores = cosine_similarity(test_vector, tfidf_matrix).flatten()\n",
        "\n",
        "    # Rank documents by similarity\n",
        "    similarity_ranking = sorted(zip(doc_names, similarity_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Calculate uniqueness as 1 - max similarity score\n",
        "    max_similarity = max(similarity_scores)\n",
        "    uniqueness = (1 - max_similarity) * 100\n",
        "\n",
        "    return uniqueness, similarity_ranking"
      ],
      "metadata": {
        "id": "qYzEA7S0xKuE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directory for training documents\n",
        "corpus_dir = \"./corpus\"\n",
        "os.makedirs(corpus_dir, exist_ok=True)\n",
        "\n",
        "# Sample text content for random document generation\n",
        "sample_sentences = [\n",
        "    \"Artificial intelligence is transforming the world in various domains.\",\n",
        "    \"Machine learning algorithms can detect patterns in data efficiently.\",\n",
        "    \"Natural language processing enables computers to understand human language.\",\n",
        "    \"Deep learning models are inspired by the structure and function of the brain.\",\n",
        "    \"Big data analytics provides insights into complex systems and trends.\",\n",
        "    \"Computer vision allows machines to interpret and process visual data.\",\n",
        "    \"Cloud computing offers scalable resources for software development.\",\n",
        "    \"Cybersecurity measures are crucial in protecting sensitive information.\",\n",
        "    \"Robotics combines mechanical engineering and artificial intelligence.\",\n",
        "    \"Blockchain technology ensures transparency and security in transactions.\"\n",
        "]\n",
        "\n",
        "def generate_random_document(num_sentences=5):\n",
        "    \"\"\"Generate a random document with a specified number of sentences.\"\"\"\n",
        "    return \" \".join(random.choices(sample_sentences, k=num_sentences))\n",
        "\n",
        "# Generate random documents\n",
        "for i in range(1, 6):  # Create 5 random documents\n",
        "    document_content = generate_random_document()\n",
        "    file_path = os.path.join(corpus_dir, f\"doc{i}.txt\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(document_content)\n",
        "\n",
        "print(f\"Random documents have been generated in the '{corpus_dir}' directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHlMohjBxXCf",
        "outputId": "b47f6213-68cd-4ce1-b3a9-370027fee003"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random documents have been generated in the './corpus' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "\n",
        "    print(\"hello world\")\n",
        "\n",
        "    # Define paths\n",
        "    corpus_dir = \"./corpus\"  # Directory containing training documents\n",
        "    test_file = \"./tests.txt\"  # Test document path\n",
        "\n",
        "    # Build index\n",
        "    print(\"Building index from training corpus...\")\n",
        "    tfidf_matrix, vectorizer, doc_names = build_index(corpus_dir)\n",
        "\n",
        "    # Load and preprocess test document\n",
        "    with open(test_file, 'r', encoding='utf-8') as file:\n",
        "        test_document = file.read()\n",
        "\n",
        "    # Perform plagiarism check\n",
        "    uniqueness, similarity_ranking = plagiarism_check(test_document, tfidf_matrix, vectorizer, doc_names)\n",
        "\n",
        "    # Output results\n",
        "    print(f\"Uniqueness of the test document: {uniqueness:.2f}%\")\n",
        "    print(\"\\nSimilarity ranking:\")\n",
        "    for rank, (doc_name, score) in enumerate(similarity_ranking, 1):\n",
        "        print(f\"{rank}. {doc_name}: Similarity Score = {score:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slMOvG6txOYe",
        "outputId": "0d9b957c-2005-4203-d32d-8f69a3a6a0cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n",
            "Building index from training corpus...\n",
            "Uniqueness of the test document: -0.00%\n",
            "\n",
            "Similarity ranking:\n",
            "1. doc1.txt: Similarity Score = 1.0000\n",
            "2. doc3.txt: Similarity Score = 0.3878\n",
            "3. doc2.txt: Similarity Score = 0.3502\n",
            "4. doc4.txt: Similarity Score = 0.3342\n",
            "5. doc5.txt: Similarity Score = 0.3042\n"
          ]
        }
      ]
    }
  ]
}